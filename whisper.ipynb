{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Testing\\Whisper\\ai-notes\\.conda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "import torchaudio\n",
    "import pyaudio\n",
    "import wave\n",
    "import ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': 0, 'structVersion': 2, 'name': 'Microsoft Sound Mapper - Input', 'hostApi': 0, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "{'index': 1, 'structVersion': 2, 'name': 'Microphone (Parsec Virtual Audi', 'hostApi': 0, 'maxInputChannels': 1, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "{'index': 2, 'structVersion': 2, 'name': 'Microphone (NVIDIA Broadcast)', 'hostApi': 0, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "{'index': 3, 'structVersion': 2, 'name': 'Microphone (Steam Streaming Mic', 'hostApi': 0, 'maxInputChannels': 8, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "{'index': 4, 'structVersion': 2, 'name': 'Microsoft Sound Mapper - Output', 'hostApi': 0, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "{'index': 5, 'structVersion': 2, 'name': 'Speakers (Steam Streaming Speak', 'hostApi': 0, 'maxInputChannels': 0, 'maxOutputChannels': 8, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "{'index': 6, 'structVersion': 2, 'name': 'Speakers (NVIDIA Broadcast)', 'hostApi': 0, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "{'index': 7, 'structVersion': 2, 'name': 'ASUS VS247 (NVIDIA High Definit', 'hostApi': 0, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "{'index': 8, 'structVersion': 2, 'name': 'Realtek Digital Output (Realtek', 'hostApi': 0, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "{'index': 9, 'structVersion': 2, 'name': 'Speakers (Steam Streaming Micro', 'hostApi': 0, 'maxInputChannels': 0, 'maxOutputChannels': 8, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "{'index': 10, 'structVersion': 2, 'name': 'Line In (Realtek HD Audio Line input)', 'hostApi': 1, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 44100.0}\n",
      "{'index': 11, 'structVersion': 2, 'name': 'Speakers (Realtek HD Audio output)', 'hostApi': 1, 'maxInputChannels': 0, 'maxOutputChannels': 8, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 44100.0}\n",
      "{'index': 12, 'structVersion': 2, 'name': 'Microphone (Realtek HD Audio Mic input)', 'hostApi': 1, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 44100.0}\n",
      "{'index': 13, 'structVersion': 2, 'name': 'SPDIF Out (Realtek HDA SPDIF Out)', 'hostApi': 1, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 44100.0}\n",
      "{'index': 14, 'structVersion': 2, 'name': 'Stereo Mix (Realtek HD Audio Stereo input)', 'hostApi': 1, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 44100.0}\n",
      "{'index': 15, 'structVersion': 2, 'name': 'Output (RTX-Audio Point)', 'hostApi': 1, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 48000.0}\n",
      "{'index': 16, 'structVersion': 2, 'name': 'Microphone (RTX-Audio Point)', 'hostApi': 1, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 48000.0}\n",
      "{'index': 17, 'structVersion': 2, 'name': 'Output ()', 'hostApi': 1, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 44100.0}\n",
      "{'index': 18, 'structVersion': 2, 'name': 'Microphone (Steam Streaming Microphone Wave)', 'hostApi': 1, 'maxInputChannels': 8, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.08533333333333333, 'defaultHighOutputLatency': 0.08533333333333333, 'defaultSampleRate': 44100.0}\n",
      "{'index': 19, 'structVersion': 2, 'name': 'Speakers (Steam Streaming Microphone Wave)', 'hostApi': 1, 'maxInputChannels': 0, 'maxOutputChannels': 8, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.08533333333333333, 'defaultHighOutputLatency': 0.08533333333333333, 'defaultSampleRate': 44100.0}\n",
      "{'index': 20, 'structVersion': 2, 'name': 'Input (Steam Streaming Speakers Wave)', 'hostApi': 1, 'maxInputChannels': 8, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.08533333333333333, 'defaultHighOutputLatency': 0.08533333333333333, 'defaultSampleRate': 44100.0}\n",
      "{'index': 21, 'structVersion': 2, 'name': 'Speakers (Steam Streaming Speakers Wave)', 'hostApi': 1, 'maxInputChannels': 0, 'maxOutputChannels': 8, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.08533333333333333, 'defaultHighOutputLatency': 0.08533333333333333, 'defaultSampleRate': 44100.0}\n",
      "{'index': 22, 'structVersion': 2, 'name': 'Microphone (Parsec Virtual Audio)', 'hostApi': 1, 'maxInputChannels': 1, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.08533333333333333, 'defaultHighOutputLatency': 0.08533333333333333, 'defaultSampleRate': 48000.0}\n"
     ]
    }
   ],
   "source": [
    "p = pyaudio.PyAudio()\n",
    "for i in range(p.get_device_count()):\n",
    "    device_info = p.get_device_info_by_index(i)\n",
    "    print(device_info)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"distil-whisper/distil-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "FORMAT = pyaudio.paInt16        # 16-bit resolution\n",
    "CHANNELS = 1                    # Mono audio\n",
    "RATE = 44100                    # 44.1kHz sampling rate\n",
    "CHUNK = 1024                    # Number of frames per buffer\n",
    "TEMP_FILENAME = \"temp_recording.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollamaFunction(content):\n",
    "  response = ollama.chat(model='llama3.1', messages=[\n",
    "    {\n",
    "      'role': 'user',\n",
    "      'content': content + \"Here\",\n",
    "    },\n",
    "  ])\n",
    "  print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_waveform(waveform, chunk_size):\n",
    "    num_chunks = waveform.size(1) // chunk_size\n",
    "    return torch.chunk(waveform, num_chunks + 1, dim=1)\n",
    "\n",
    "def preprocess_audio(waveform, processor, sample_rate=16000):\n",
    "    input_features = processor(\n",
    "        waveform.squeeze().numpy(), sampling_rate=sample_rate, return_tensors=\"pt\"\n",
    "    ).input_features\n",
    "    return input_features\n",
    "\n",
    "def generate_transcription(model, processor, input_features, torch_dtype=torch.float32):\n",
    "    # Convert input features to the appropriate dtype if necessary\n",
    "    input_features = input_features.to(torch_dtype)\n",
    "    \n",
    "    # Ensure model is also in the appropriate dtype\n",
    "    model = model.to(torch_dtype)\n",
    "    \n",
    "    # Generate transcription\n",
    "    predicted_ids = model.generate(input_features)\n",
    "    return processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "def audio_process(wav_file_path, model, processor, torch_dtype=torch.float32, device=\"cpu\"):\n",
    "    waveform, sample_rate = torchaudio.load(wav_file_path)\n",
    "\n",
    "    if sample_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    chunk_size = 16000 * 30  # 30 seconds at 16000 Hz\n",
    "    waveform_chunks = chunk_waveform(waveform, chunk_size)\n",
    "\n",
    "    transcription = []\n",
    "\n",
    "    for chunk in waveform_chunks:\n",
    "        input_features = preprocess_audio(chunk, processor, sample_rate=16000)\n",
    "\n",
    "        # Convert input features to the appropriate device and dtype\n",
    "        input_features = input_features.to(device).to(torch_dtype)\n",
    "\n",
    "        # Ensure model is in the appropriate device and dtype\n",
    "        model = model.to(device).to(torch_dtype)\n",
    "\n",
    "        chunk_transcription = generate_transcription(model, processor, input_features, torch_dtype)\n",
    "\n",
    "        transcription.append(chunk_transcription)\n",
    "\n",
    "    return \" \".join(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "# Constants\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 44100\n",
    "CHUNK = 1024\n",
    "OUTPUT_FILENAME = \"output.wav\"\n",
    "\n",
    "def record_audio():\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK)\n",
    "\n",
    "    print(\"Recording started. Press Ctrl+C to stop.\")\n",
    "    frames = []\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            data = stream.read(CHUNK)\n",
    "            frames.append(data)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nRecording stopped by user.\")\n",
    "    finally:\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "\n",
    "        with wave.open(OUTPUT_FILENAME, 'wb') as wf:\n",
    "            wf.setnchannels(CHANNELS)\n",
    "            wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "            wf.setframerate(RATE)\n",
    "            wf.writeframes(b''.join(frames))\n",
    "\n",
    "        return(audio_process(OUTPUT_FILENAME))\n",
    "       \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openNotes(fileName):\n",
    "  with open(fileName, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "  processed_lines = []\n",
    "  for line in lines:\n",
    "      stripped_line = line.strip()  # Remove leading/trailing whitespace\n",
    "      if stripped_line:\n",
    "          # If the line is not empty, check if it's the start of a new section\n",
    "          if stripped_line.endswith(\":\"):  # Assuming sections might end with a colon\n",
    "              processed_lines.append(\"\\n\\n\" + stripped_line)\n",
    "          else:\n",
    "              processed_lines.append(stripped_line)\n",
    "      else:\n",
    "          # Insert a double newline for paragraph breaks or section ends\n",
    "          processed_lines.append(\"\\n\\n\")\n",
    "\n",
    "  # Join lines, ensuring that excess newlines are handled\n",
    "  notes = ' '.join(processed_lines).replace('\\n\\n ', '\\n\\n').replace('  ', ' ')\n",
    "\n",
    "  # Optional: Further cleaning up and normalizing spacing\n",
    "  return (notes.strip().replace('\\n\\n', '[SECTION]').replace('\\n', ' ').replace('[SECTION]', '\\n\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started. Press Ctrl+C to stop.\n",
      "\n",
      "Recording stopped by user.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "audio_process() missing 2 required positional arguments: 'model' and 'processor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrecord_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 35\u001b[0m, in \u001b[0;36mrecord_audio\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m     wf\u001b[38;5;241m.\u001b[39msetframerate(RATE)\n\u001b[0;32m     33\u001b[0m     wf\u001b[38;5;241m.\u001b[39mwriteframes(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(frames))\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(\u001b[43maudio_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_FILENAME\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: audio_process() missing 2 required positional arguments: 'model' and 'processor'"
     ]
    }
   ],
   "source": [
    "record_audio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the combined notes in bullet point form:\n",
      "\n",
      "**Key Discussion Points:**\n",
      "\n",
      "* **GUI Upgrade**: The client wants to upgrade the website's GUI (Graphical User Interface) to something nicer.\n",
      "* **Shop Integration**: They'd like to integrate a shop into the website, with an understanding that extra work will be required for this feature.\n",
      "\n",
      "**Decisions Made:**\n",
      "\n",
      "* **Pricing**: A budget of $4,000 was agreed upon for the upgrades and shop integration.\n",
      "* **Theme Customization**: The client expressed love for the current blue theme, but wants to add some green accents. This will require discussion with designers.\n",
      "\n",
      "**Action Items/ Follow-ups:**\n",
      "\n",
      "* **Design Meeting**: Schedule a meeting with designers to discuss adding green accents to the website's design.\n",
      "* **Pricing Discussion**: Further discussion on pricing is needed to ensure that the client understands the value of the work being done.\n",
      "* **Meeting Scheduled**: A meeting has been scheduled for tomorrow at 2 o'clock to further discuss the project.\n"
     ]
    }
   ],
   "source": [
    "notes = openNotes(\"notes.txt\")\n",
    "content = (audio_process(\"output.wav\", processor = processor, model = model, device = device))    \n",
    "#print(audio_process(\"output.wav\", processor = processor, model = model, device = device))\n",
    "#content = \n",
    "ollamaFunction(\"Instructions: 'Create realistic notes from the following conversation. The notes should summarize the key points discussed, include any decisions made, and highlight action items or follow-ups. The notes should be concise but comprehensive, capturing the essence of the dialogue. Please keep this context in mind as you process each 30-second segment of the conversation.' here is the voice notes: '\" + content + \"' Here are my notes from the meeting: '\" + notes + \"' Can you combine the notes with the voicechat but keep the notes more relevant and more of a prescence, and also make it into bullet points if it makes scence.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
